-nltk tokenizer isn't that good, consider switching to splitta
-maybe stanford force tokenizes when it tags?

  TODO:
    - get gensim installed on server
      - try wordvec using text8 to debug
      - move google news to /p/cl
    - lesk using pywsd
    - LSA using wiki corpus


March 12 -----------------------------------------------------------------------
  Completed:
    - installed gensim! yay!
    - word2vec code is integrated (same approach as for wordnet similarity:
    take max similarity score between word combinations), but model is too slow
    to load when using the google news dataset
    - wiki corpus for LSA failed to make. trying again but very slow.
    but thinking I don't need the wiki corpus -> make a mini corpus for each
    possible antecedent, compare anaphor to this corpus

  TODO:
    - LSA scores
    - useable word2vec model
    - lesk (pywsd?)

March 5 -----------------------------------------------------------------------
  Completed:
    - take the longest compound word, if there are subsets
    - finished normalizing scores (lch, resnick, jiang-conrath)
    - find most similar antecedent based on highest average similarity score
    - make repo

  TODO:
    - LSA scores
    - lesk
    - word2vec (can use from gensim) https://code.google.com/p/word2vec/

  server to try
  eniac.ais.sandbox

Feb 25 -----------------------------------------------------------------------
  Completed:
    - ignore prepositions and pronouns and punctuation (stoplist)
      - def remove_stopwords, called from tag() which does all the POS tagging

    - compound words, consider as a whole not parts
      - def find_compound_words

    - normalize scores (started, not completely done)
      - Averages scores with a [0, 1] range -> path, wup, lin

  TODO:
    - normalize scores for lch, resnick, jiang-conrath (non-[0,1]-range scores)
    - LSA scores
    - lesk
    - word2vec (can use from gensim)
    - make repo

  meeting notes:
    - take the longest compound word, if there are subsets
    - word2vec (can use from gensim)

  Normalization notes:
    -path: Ranges from 0 to 1, based on shortest is-a path
      - score = 1/(shortest is-a path length)
    -leacock-chodorow: 
      - score = -log(shortest path between synsets/2*depth of taxonomy)
      - upper limit is − ln(1/(2 · maxDepth))  = 3.6889 in wordnet 
        - maxDepth is a maximum depth of the taxonomy
      - http://stackoverflow.com/questions/20112828/maximum-score-in-wordnet-based-similarity
    -wu-palmer: Ranges from 0 to 1, based on depth of the words and their LCS
      - score = 2*depth(lcs) / (depth(s1) + depth(s2))
    -resnick: >= 0, based on the information content of the LCS
      - score = IC(LCS) = -log(p(lcs))
      - upper limit is log(N) where N=number of words in corpus
        - 19.95207 for 1014312 words in Brown corpus and log base 2
        - nunif strategy
    -jiang-conrath:
      - score = 1 / (IC(s1) + IC(s2) - 2 * IC(lcs))
      - upper limit (identical words) is -1/(ln((f_root - 1)/f_root))
        - largest for large values of f_root (frequency of the taxonomy root)
    -lin: 
      - score = 2 * IC(lcs) / (IC(s1) + IC(s2))
      - Ranges from 0 to 1

    0 to 1: Path, wup, lin
    Other: lch, resnick, jiang-conrath


Feb 12 ------------------------------------------------------------------------
  Completed:
    - lemmatize (stemming) to get root form of a word --> wordnet should do this
      - from nltk.stem.wordnet import WordNetLemmatizer
        lmtzr = WordNetLemmatizer()
    - function to compare all combinations of synsets (couldn't find pre-written one)
      - compares all synsets for a word from sentence 1 to all synsets of all words in sentence 2. saves max score
      - returns average max score (sum of max scores/number of words in s1)
    - function to find sim score between the first sense of 

  Mention:
    - skip meeting next week?
    - difference in number of tokens being compared

  TODO:
    - normalize scores
    - LSA scores
    - compound words, consider as a whole not parts -> check each sentence against compound word list in wordnet
    - ignore prepositions and pronouns and punctuation (stoplist)
    - lesk
    - make repo

  meeting notes:
    def tag:
      - does stnaford tagger need tokenized words?
      - splitta tokenizer

    - add adverbs, adjectives
    - VerbNet??


Feb 05 ------------------------------------------------------------------------

Questions:
  -difference between what I am doing and SEMILAR (http://www.semanticsimilarity.org/) and TakeLab (http://aclweb.org/anthology//S/S12/S12-1060.pdf)

  -group all nouns together? ie NN, NNS, NNP, NNPS
    YES
  -stop words? ignore all prepositions, pronouns, punctuation

  Proposed TODO for next week: generate sentence similarity scores from wordnet similarity scores
  - normalize?

Meeting:
  -lesk
  -can just use 1st sense since it's the most frequent
  -or can consider all of them and take max similarity (lean towards this)
    -see if a function already exists that will give word similairty instead of sense similarity (ie consider all synsets of a word)
  -ignore prepositions and pronouns and punctuation (stoplist)
  -lemmatize (stemming) to get root form of a word --> wordnet should do this
  -compound words, consider as a whole not parts -> check each sentence against compound word list in wordnet
  -make repo in cs.toronto.edu

Jan 29 ------------------------------------------------------------------------

Meeting:

  Use word similarity (all words, all verbs, all nouns) to generate sentence similarity scores.
  ignore stop words ()

  **lsa least semantic analysis measure using GENSIM

  Part of speech tagging (Stanford parser)

  Normalize scores 

  Can't use path measures for verbs to noun. >> vector, lesk work

  try using all similarity measures on V-V, N-N, and overall
